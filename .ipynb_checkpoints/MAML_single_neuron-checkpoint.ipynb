{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154517e4-9cdf-4276-967e-3593a8c52617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 17:14:29.700501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Model's initial weights before inner loop are\n",
      "[[1.0]\n",
      " [-4.0]]\n",
      "Model output is: [[0.04742587]]\n",
      "Support loss is 3.0486\n",
      "Inner loop gradients are\n",
      "[[-0.9525721073150635]\n",
      " [-0.9525721073150635]]\n",
      "Inner model's weights after inner gradient step are\n",
      "[[1.9525721073150635]\n",
      " [-3.0474278926849365]]\n",
      "Query loss is 1.3834779262542725\n",
      "Outer loop meta gradients are\n",
      "[[-0.6815964579582214]\n",
      " [-0.6815964579582214]]\n",
      "Meta Model's updated weights after outer loop adaptation are\n",
      "[[1.6815965175628662]\n",
      " [-3.318403482437134]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 17:14:35.058852: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-03-15 17:14:35.083560: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/mvapich2-2.3.7-1-qv3gjagtbx5e3rlbdy6iy2sfczryftyt/lib:/opt/slurm/current/lib:/opt/pmix/4.2.6/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/libpciaccess-0.17-pjfe4ct4gfm5k26s36hmewhbz4k232dl/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/gcc-12.3.0-ukkkutsxfl5kpnnaxflpkq2jtliwthfz/lib64:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/gcc-12.3.0-ukkkutsxfl5kpnnaxflpkq2jtliwthfz/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/zlib-1.2.13-3qhjpij2pji47kfanmlflvwk5ljcn5lh/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/mpc-1.3.1-3zvixith5i243hwiill4exyu3lf2q6ad/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/mpfr-4.2.0-tox4bdsc25zr763rnq5gzlukfbvw7uj7/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/gmp-6.2.1-n7dzsse5e3f6w5z6q6cuqursydg6yypo/lib::\n",
      "2025-03-15 17:14:35.083860: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-15 17:14:35.084022: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login-phoenix-rh9-2.pace.gatech.edu): /proc/driver/nvidia/version does not exist\n",
      "2025-03-15 17:14:35.086145: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-15 17:14:35.088517: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "/tmp/ipykernel_2173336/4145080991.py:79: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(self.model.trainable_weights))\n",
      "/tmp/ipykernel_2173336/4145080991.py:87: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(grads))\n",
      "/tmp/ipykernel_2173336/4145080991.py:90: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(weights))\n",
      "/tmp/ipykernel_2173336/4145080991.py:103: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(meta_gradients))\n",
      "/tmp/ipykernel_2173336/4145080991.py:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(np.array(self.model.trainable_variables))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import tracemalloc\n",
    "import random\n",
    "import copy\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras import metrics\n",
    "from random import shuffle\n",
    "from copy import deepcopy, copy\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "num_classes = 2  # Example binary classification\n",
    "metatrain_iter = 1  # Number of MAML updates\n",
    "innertrain_iter = 1  # Number of inner gradient updates\n",
    "inner_lr = 1    # Inner loop learning rate\n",
    "meta_lr = 1    # Meta-learning rate\n",
    "num_tasks = 1  #Number of sampled tasks for meta learning\n",
    "\n",
    "# Define a simple model with one Dense layer\n",
    "class SingleNeuron(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SingleNeuron, self).__init__()\n",
    "        self.layer = tf.keras.layers.Dense(\n",
    "            units=1,  # One output node\n",
    "            activation=tf.keras.activations.sigmoid,  # No activation function\n",
    "            # activation=None,  # No activation function\n",
    "            use_bias=True  # Include bias\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return self.layer(inputs)\n",
    "\n",
    "def fastWeights(model, weights, input):\n",
    "    output = input\n",
    "    for layerIndex in range(len(model.layers)):\n",
    "        kernel = weights[layerIndex * 2]\n",
    "        bias = weights[layerIndex * 2 + 1]\n",
    "        output = model.layers[layerIndex].activation(output @ kernel + bias)\n",
    "    return output\n",
    "\n",
    "class MAML:\n",
    "    def __init__(self, input_shape=(1), num_classes=2, inner_lr=1, outer_lr=1, \n",
    "                 inner_steps=1, outer_steps=1, num_users=1, support_size=20, query_size=20):\n",
    "        self.inner_steps = inner_steps\n",
    "        self.outer_steps = outer_steps\n",
    "        self.num_classes = num_classes\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.support_size = support_size\n",
    "        self.query_size = query_size\n",
    "        self.num_users = num_users\n",
    "        self.model = self.create_model(num_classes)\n",
    "        self.meta_optimizer = tf.keras.optimizers.SGD(learning_rate=outer_lr)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    def create_model(self,num_classes):\n",
    "        model = SingleNeuron()   \n",
    "        model.build(input_shape=(None, 1))  # Ensure the model is built\n",
    "        model.layers[0].set_weights([tf.constant([[1.0]]), tf.constant([-4.0])])\n",
    "        return model\n",
    "    \n",
    "    def get_weights(self, model):\n",
    "        \"\"\"Create a copy of model weights\"\"\"\n",
    "        copied_weights = [tf.identity(w) for w in model.trainable_variables]\n",
    "        return copied_weights\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"Set model weights to the given weights\"\"\"\n",
    "        for w_model, w_new in zip(self.model.trainable_variables, weights):\n",
    "            w_model.assign(w_new)\n",
    "\n",
    "    def updateMAML(self):\n",
    "        def taskLoss(batch):\n",
    "            support_x,support_y,query_x,query_y = batch            \n",
    "            print(\"Inner Model's initial weights before inner loop are\")\n",
    "            print(np.array(self.model.trainable_weights))\n",
    "            with tf.GradientTape() as taskTape:\n",
    "                print(f\"Model output is: {np.array(self.model(support_x))}\") \n",
    "                loss = self.loss_fn(support_y, self.model(support_x))\n",
    "                print(f\"Support loss is {loss.numpy():.4f}\")\n",
    "            \n",
    "            grads = taskTape.gradient(loss, self.model.trainable_weights)\n",
    "            print(\"Inner loop gradients are\")\n",
    "            print(np.array(grads))\n",
    "            weights = [w - self.inner_lr * g for g, w in zip(grads, self.model.trainable_weights)]\n",
    "            print(\"Inner model's weights after inner gradient step are\")\n",
    "            print(np.array(weights))\n",
    "            \n",
    "            return self.loss_fn(query_y, fastWeights(self.model, weights, query_x))\n",
    "    \n",
    "        batch=[tf.Variable([[1.0]]),tf.constant([1.0]),tf.Variable([[1.0]]),tf.constant([1.0])]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # loss = tf.map_fn(taskLoss, elems=batch,fn_output_signature=tf.float32)\n",
    "            # loss = tf.reduce_sum(batchLoss)\n",
    "            loss = taskLoss(batch)\n",
    "        \n",
    "        print(\"Meta Model's weights before outer loop adaptation are\")\n",
    "        print(np.array(self.model.trainable_variables))\n",
    "        print(f\"Query loss is {loss}\")\n",
    "        meta_gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        print(\"Outer loop meta gradients are\")\n",
    "        print(np.array(meta_gradients))\n",
    "        self.meta_optimizer.apply_gradients(zip(meta_gradients, self.model.trainable_variables))\n",
    "        print(\"Meta Model's updated weights after outer loop adaptation are\")\n",
    "        print(np.array(self.model.trainable_variables))\n",
    "        \n",
    "maml = MAML()\n",
    "meta_model = maml.updateMAML()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
