{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154517e4-9cdf-4276-967e-3593a8c52617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 18:11:57.981628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-03-15 18:12:03.718010: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-03-15 18:12:03.719901: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/mvapich2-2.3.7-1-qv3gjagtbx5e3rlbdy6iy2sfczryftyt/lib:/opt/slurm/current/lib:/opt/pmix/4.2.6/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/libpciaccess-0.17-pjfe4ct4gfm5k26s36hmewhbz4k232dl/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/gcc-12.3.0-ukkkutsxfl5kpnnaxflpkq2jtliwthfz/lib64:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-11.3.1/gcc-12.3.0-ukkkutsxfl5kpnnaxflpkq2jtliwthfz/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/zlib-1.2.13-3qhjpij2pji47kfanmlflvwk5ljcn5lh/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/mpc-1.3.1-3zvixith5i243hwiill4exyu3lf2q6ad/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/mpfr-4.2.0-tox4bdsc25zr763rnq5gzlukfbvw7uj7/lib:/usr/local/pace-apps/spack/packages/linux-rhel9-x86_64_v3/gcc-12.3.0/gmp-6.2.1-n7dzsse5e3f6w5z6q6cuqursydg6yypo/lib::\n",
      "2025-03-15 18:12:03.720082: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-03-15 18:12:03.720261: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (login-phoenix-rh9-2.pace.gatech.edu): /proc/driver/nvidia/version does not exist\n",
      "2025-03-15 18:12:03.721997: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-15 18:12:03.729543: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6933\n",
      "Step 0/5 completed\n",
      "Train Loss: 0.6921\n",
      "Step 1/5 completed\n",
      "Train Loss: 0.6912\n",
      "Step 2/5 completed\n",
      "Train Loss: 0.6903\n",
      "Step 3/5 completed\n",
      "Train Loss: 0.6895\n",
      "Step 4/5 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 18:12:12.279900: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2025-03-15 18:12:12.298966: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2700000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5012\n",
      "Test accuracy: 0.5148\n",
      "Train Loss: 0.6937\n",
      "Step 0/5 completed\n",
      "Train Loss: 0.6922\n",
      "Step 1/5 completed\n",
      "Train Loss: 0.6912\n",
      "Step 2/5 completed\n",
      "Train Loss: 0.6902\n",
      "Step 3/5 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2188507/3486222941.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracemalloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2188507/3486222941.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, user_data, train_user, test_users, pretraining_epochs, pretrain_lr, pretrain)\u001b[0m\n\u001b[1;32m     73\u001b[0m         self.model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=pretrain_lr),\n\u001b[1;32m     74\u001b[0m           metrics=[metrics.SpecificityAtSensitivity(0.5, num_thresholds=50)])\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateMAML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Step {step}/{self.outer_steps} completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2188507/3486222941.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, user_data, test_users)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;31m# loss = tf.map_fn(taskLoss, elems=(support_x,support_y,query_x,query_y),fn_output_signature=tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# loss = tf.reduce_sum(batchLoss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtaskLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msupport_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msupport_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mmeta_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {avg_train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_gradients\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     raise ValueError(\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    618\u001b[0m           \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m           data_format=op.get_attr(\"data_format\")),\n\u001b[0;32m--> 622\u001b[0;31m       gen_nn_ops.depthwise_conv2d_native_backprop_filter(\n\u001b[0m\u001b[1;32m    623\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m           \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-gpu/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2342\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2343\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2345\u001b[0m       return depthwise_conv2d_native_backprop_filter_eager_fallback(\n\u001b[1;32m   2346\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from EEGNet_def import EEGNet, fastWeights_EEGNet\n",
    "from utils import generate_user_data\n",
    "import pickle\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import clone_model\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import tracemalloc\n",
    "import random\n",
    "from random import shuffle\n",
    "from copy import deepcopy, copy\n",
    "from tensorflow.keras import utils as np_utils\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "class MAML:\n",
    "    def __init__(self, model_params, input_shape=(64, 128, 1), num_classes=2, inner_lr=0.001, outer_lr=0.001, \n",
    "                 inner_steps=5, outer_steps=100, fine_tune_steps = 10, num_users=5, support_size=20, query_size=20):\n",
    "        self.inner_steps = inner_steps\n",
    "        self.outer_steps = outer_steps\n",
    "        self.fine_tune_steps = fine_tune_steps\n",
    "        self.num_classes = num_classes\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.support_size = support_size\n",
    "        self.query_size = query_size\n",
    "        self.num_users = num_users\n",
    "        self.model = self.create_eegnet(num_classes, model_params)\n",
    "        self.meta_optimizer = tf.keras.optimizers.Adam(outer_lr)\n",
    "        self.loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    def create_eegnet(self,num_classes,model_params):\n",
    "        num_channels, num_time_samples, model_dropout, model_kern, model_f1, model_d, model_f2 = model_params\n",
    "        eegnet = EEGNet(nb_classes=num_classes,Chans=num_channels, Samples=num_time_samples, dropoutRate = model_dropout, \n",
    "                kernLength = model_kern, F1 = model_f1, D = model_d, F2 = model_f2)  \n",
    "        eegnet.build((None, num_channels, num_time_samples,1))\n",
    "        return eegnet\n",
    "                \n",
    "    def updateMAML(self, user_data, test_users):\n",
    "        def taskLoss(batch):\n",
    "            support_x,support_y,query_x,query_y = batch\n",
    "            with tf.GradientTape() as taskTape:\n",
    "                loss = self.loss_fn(support_y, self.model(support_x))\n",
    "            \n",
    "            grads = taskTape.gradient(loss, self.model.trainable_weights)\n",
    "            weights = [w - self.inner_lr * g for g, w in zip(grads, self.model.trainable_weights)]\n",
    "            return self.loss_fn(query_y, fastWeights_EEGNet(self.model, weights, query_x))\n",
    "    \n",
    "        total_train_loss=0\n",
    "        test_acc_list = [[] for i in range(len(test_users))]\n",
    "        for idx,user in enumerate(test_users):\n",
    "            support_x, support_y = user_data[user][\"support\"]\n",
    "            query_x, query_y = user_data[user][\"query\"]\n",
    "            test_x, test_y = user_data[user][\"test\"]\n",
    "            with tf.GradientTape() as tape:\t\n",
    "                # loss = tf.map_fn(taskLoss, elems=(support_x,support_y,query_x,query_y),fn_output_signature=tf.float32)\n",
    "                # loss = tf.reduce_sum(batchLoss)\n",
    "                loss = taskLoss([support_x,support_y,query_x,query_y])\n",
    "                total_train_loss+=loss\n",
    "            meta_gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.meta_optimizer.apply_gradients(zip(meta_gradients, self.model.trainable_variables))\n",
    "        avg_train_loss = total_train_loss / (len(test_users))\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        return avg_train_loss\n",
    "\n",
    "    def train(self, user_data, train_user, test_users, pretraining_epochs=100, pretrain_lr=0.001, pretrain=False):\n",
    "        train_loss_list=[]\n",
    "        self.model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=pretrain_lr),\n",
    "          metrics=[metrics.SpecificityAtSensitivity(0.5, num_thresholds=50)])\n",
    "        self.model.summary\n",
    "        for step in range(self.outer_steps):\n",
    "            train_loss_list.append(self.updateMAML(user_data, test_users))\n",
    "            if step % 1 == 0:\n",
    "                print(f\"Step {step}/{self.outer_steps} completed\")\n",
    "        return self.model\n",
    "\n",
    "    def test_model(self, model_params, user_data, test_users):\n",
    "        acc_fine_tune=[]\n",
    "        for user in test_users:\n",
    "            user_model = self.create_eegnet(self.num_classes, model_params)\n",
    "            user_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "            user_model.load_weights('model_weights/maml/model')\n",
    "            support_x, support_y = user_data[user][\"support\"]\n",
    "            test_data, test_labels = user_data[user][\"test\"]\n",
    "            test_labels = tf.cast(test_labels, tf.int64)\n",
    "            test_labels = tf.argmax(test_labels, axis=1)  # Convert categorical to class index\n",
    "\n",
    "            # Fine-tune on the support set\n",
    "            user_model.fit(support_x, support_y, epochs=self.fine_tune_steps, verbose=0)\n",
    "            outputs = user_model(test_data)\n",
    "            predicted = tf.argmax(outputs, axis=1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, test_labels), tf.float32))\n",
    "            acc_fine_tune.append(accuracy.numpy())\n",
    "        print(f\"Test accuracy: {np.mean(acc_fine_tune):.4f}\")\n",
    "\n",
    "num_classes = 2  # Example binary classification\n",
    "num_users = 10  \n",
    "num_samples = 400  \n",
    "num_channels = 12  \n",
    "num_time_samples = 100  \n",
    "support_size = 40  # Number of support samples per user\n",
    "query_size = 40    # Number of query samples per user\n",
    "model_epochs=100\n",
    "model_dropout=0.4\n",
    "model_kern=16\n",
    "model_f1=16\n",
    "model_f2=16\n",
    "model_d=4\n",
    "metatrain_iter = 5  # Number of MAML updates\n",
    "innertrain_iter = 5  # Number of inner gradient updates\n",
    "fine_tune_steps = 5  # Number of iterations for fine-tuning\n",
    "pretrain_lr = 0.001    # Inner loop learning rate\n",
    "inner_lr = 0.001    # Inner loop learning rate\n",
    "meta_lr = 0.001    # Meta-learning rate\n",
    "num_tasks = 5\n",
    "\n",
    "user_data = generate_user_data(num_users, num_samples, num_channels, num_time_samples, support_size, query_size)\n",
    "model_params = [num_channels, num_time_samples, model_dropout, model_kern, model_f1, model_d, model_f2]\n",
    "\n",
    "keys = list(user_data.keys())\n",
    "for i in range(num_users):\n",
    "    test_users = [x for x in keys if x != i]\n",
    "    shuffled_users = random.sample(test_users, len(test_users))\n",
    "    meta_users = shuffled_users[:num_tasks]\n",
    "    unseen_users = shuffled_users[num_tasks:]\n",
    "    maml = MAML(model_params, input_shape=(num_channels, num_time_samples, 1), num_classes=num_classes, inner_lr=inner_lr,\n",
    "                outer_lr=meta_lr, inner_steps=innertrain_iter, outer_steps=metatrain_iter, fine_tune_steps = fine_tune_steps,\n",
    "                num_users=num_tasks, support_size=support_size, query_size=query_size)\n",
    "    meta_model = maml.train(user_data,i,meta_users,pretrain_lr=pretrain_lr, pretrain=False)\n",
    "    meta_model.save_weights('model_weights/maml/model')\n",
    "    maml.test_model(model_params, user_data, meta_users)\n",
    "    maml.test_model(model_params, user_data, unseen_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
